{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "API_BASE = \"https://fair2-dev.lutechdigitale.it/med-gemma\"\n",
    "PREDICTION_API_BASE = \"https://fair2-dev.lutechdigitale.it/data-provisioning-brainmed\"\n",
    "\n",
    "# Configurazioni predefinite ottimizzate\n",
    "CONFIGS = {\n",
    "    'conservative': {\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.3,\n",
    "        'max_tokens': 500,\n",
    "        'do_sample': True,\n",
    "        'n_runs': 1,\n",
    "        'validation_threshold': 40\n",
    "    },\n",
    "    'balanced': {\n",
    "        'temperature': 0.2,\n",
    "        'top_p': 0.5,\n",
    "        'max_tokens': 700,\n",
    "        'do_sample': True,\n",
    "        'n_runs': 1,\n",
    "        'validation_threshold': 50\n",
    "    },\n",
    "    'creative': {\n",
    "        'temperature': 0.3,\n",
    "        'top_p': 0.7,\n",
    "        'max_tokens': 700,\n",
    "        'do_sample': True,\n",
    "        'n_runs': 1,\n",
    "        'validation_threshold': 45\n",
    "    }\n",
    "}\n",
    "\n",
    "# Usa configurazione conservativa per default (più stabile)\n",
    "CURRENT_CONFIG = CONFIGS['conservative']\n",
    "\n",
    "# ==================== PREDICTION API SERVICE ====================\n",
    "\n",
    "class BrainMedPredictionService:\n",
    "    def __init__(self, base_url: str = PREDICTION_API_BASE):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def get_patient_prediction(self, patient_id: str) -> dict | None:\n",
    "        \"\"\"\n",
    "        Recupera le predizioni e metriche per un paziente specifico\n",
    "        \n",
    "        Args:\n",
    "            patient_id: ID del paziente\n",
    "            \n",
    "        Returns:\n",
    "            Dict con predizioni e metriche o None se errore\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/prediction_model/{patient_id}/latest\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Errore nella chiamata API predizioni: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def format_prediction_for_prompt(self, prediction_data: dict, include_metrics: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Formatta i dati di predizione per il prompt AI\n",
    "        \n",
    "        Args:\n",
    "            prediction_data: Dati dalla API\n",
    "            include_metrics: Se includere le metriche nel prompt\n",
    "            \n",
    "        Returns:\n",
    "            Stringa formattata per il prompt\n",
    "        \"\"\"\n",
    "        if not prediction_data:\n",
    "            return \"❌ Nessuna predizione disponibile per questo paziente.\"\n",
    "        \n",
    "        # Mappiamo la classe della malattia a una descrizione\n",
    "        disease_classes = {\n",
    "            0: \"Nessuna patologia rilevata (CN - Cognitivamente Normale)\",\n",
    "            1: \"Patologia rilevata (possibile MCI o AD)\",\n",
    "            # Aggiungi altre classi se necessario\n",
    "        }\n",
    "        \n",
    "        class_disease = prediction_data.get('class_disease', 'N/A')\n",
    "        disease_description = disease_classes.get(class_disease, f\"Classe {class_disease}\")\n",
    "        \n",
    "        prompt_text = f\"\"\"\n",
    "🤖 PREDIZIONE MODELLO AI:\n",
    "• Classe predetta: {disease_description}\n",
    "• Valore numerico: {class_disease}\n",
    "• Data predizione: {prediction_data.get('date_time', 'N/A')}\n",
    "• ID paziente: {prediction_data.get('id', 'N/A')}\n",
    "\"\"\"\n",
    "        \n",
    "        if include_metrics:\n",
    "            accuracy = prediction_data.get('accuracy', 0)\n",
    "            precision = prediction_data.get('precision', 0)\n",
    "            recall = prediction_data.get('recall', 0)\n",
    "            f1_score = prediction_data.get('f1_score', 0)\n",
    "            \n",
    "            prompt_text += f\"\"\"\n",
    "📊 METRICHE DEL MODELLO:\n",
    "• Accuracy: {accuracy:.1%} (precisione generale)\n",
    "• Precision: {precision:.1%} (quando predice positivo, quanto è accurato)\n",
    "• Recall: {recall:.1%} (quanti casi positivi cattura)\n",
    "• F1 Score: {f1_score:.1%} (bilanciamento precision/recall)\n",
    "\"\"\"\n",
    "        \n",
    "        return prompt_text.strip()\n",
    "\n",
    "# ==================== EXISTING FUNCTIONS ====================\n",
    "\n",
    "def predict_image(\n",
    "    image_path: str,\n",
    "    prompt: str,\n",
    "    *,\n",
    "    max_tokens: int = 500,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.3,\n",
    "    num_beams: int | None = None,\n",
    "    do_sample: bool = True,\n",
    "    poll_interval: int = 5,\n",
    "    timeout: int = 900,\n",
    "):\n",
    "    \"\"\"Funzione per chiamare l'API di predizione\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        files = {\"file\": (image_path, f, \"application/octet-stream\")}\n",
    "        data = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": str(max_tokens),\n",
    "            \"temperature\": str(temperature),\n",
    "            \"top_p\": str(top_p),\n",
    "            **({\"num_beams\": str(num_beams)} if num_beams is not None else {}),\n",
    "            \"do_sample\": str(do_sample).lower(),\n",
    "        }\n",
    "\n",
    "        resp = requests.post(\n",
    "            f\"{API_BASE}/predict_image\",\n",
    "            files=files,\n",
    "            data=data,\n",
    "            timeout=(10, 30),\n",
    "        )\n",
    "    resp.raise_for_status()\n",
    "    job_id = resp.json()[\"job_id\"]\n",
    "\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        if time.time() - start > timeout:\n",
    "            raise TimeoutError(\"Timeout waiting for job to finish\")\n",
    "\n",
    "        status_resp = requests.get(\n",
    "            f\"{API_BASE}/jobs/{job_id}\", timeout=(5, 10)\n",
    "        )\n",
    "        status_resp.raise_for_status()\n",
    "        payload = status_resp.json()\n",
    "        status = payload[\"status\"]\n",
    "\n",
    "        if status == \"pending\":\n",
    "            time.sleep(poll_interval)\n",
    "            continue\n",
    "        elif status == \"done\":\n",
    "            return payload[\"result\"]\n",
    "        else:\n",
    "            raise RuntimeError(f\"Job failed: {payload.get('detail')}\")\n",
    "\n",
    "def clean_response_v2(text: str) -> str:\n",
    "    \"\"\"Pulisce la risposta da ripetizioni e artefatti\"\"\"\n",
    "    # Rimuovi ripetizioni del prompt\n",
    "    if \"Do not repeat these instructions\" in text:\n",
    "        text = text.split(\"Do not repeat these instructions\")[0]\n",
    "    \n",
    "    # Rimuovi ripetizioni evidenti (stesso paragrafo ripetuto)\n",
    "    lines = text.split('\\n')\n",
    "    seen_lines = set()\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and line not in seen_lines:\n",
    "            seen_lines.add(line)\n",
    "            cleaned_lines.append(line)\n",
    "        elif not line:  # Mantieni righe vuote\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    # Trova inizio analisi se presente\n",
    "    start_markers = [\"Primary Findings:\", \"What I see:\", \"Describe what you see\", \"Findings:\", \"OSSERVAZIONI CONCRETE:\", \"COSA VEDO:\"]\n",
    "    start_index = -1\n",
    "    \n",
    "    for marker in start_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            start_index = idx\n",
    "            break\n",
    "    \n",
    "    if start_index != -1:\n",
    "        # Cerca ripetizioni del marker\n",
    "        second_index = text.find(marker, start_index + 20)\n",
    "        if second_index != -1:\n",
    "            return text[start_index:second_index].strip()\n",
    "        return text[start_index:].strip()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def validate_actual_analysis(result: str) -> bool:\n",
    "    \"\"\"Valida che il risultato contenga analisi reale, non template\"\"\"\n",
    "    # Verifica che non sia solo template\n",
    "    template_indicators = [\n",
    "        '[Specific finding',\n",
    "        '[Describe',\n",
    "        '[Identify',\n",
    "        '[High/Medium/Low]',\n",
    "        '[Immediate clinical',\n",
    "        '[Clinical significance',\n",
    "        '[Activation intensity'\n",
    "    ]\n",
    "    \n",
    "    # Se contiene troppi indicatori di template, è fallito\n",
    "    template_count = sum(1 for indicator in template_indicators if indicator in result)\n",
    "    if template_count > 1:\n",
    "        return False\n",
    "    \n",
    "    # Verifica che non sia ripetitivo\n",
    "    lines = result.split('\\n')\n",
    "    unique_lines = set(line.strip() for line in lines if line.strip())\n",
    "    if len(lines) > 10 and len(unique_lines) < len(lines) * 0.7:\n",
    "        return False\n",
    "    \n",
    "    # Verifica presenza di contenuto medico specifico\n",
    "    medical_content = [\n",
    "        'activation', 'cortex', 'hippocampus', 'temporal', 'frontal',\n",
    "        'red', 'blue', 'high', 'low', 'medium', 'pattern', 'region',\n",
    "        'sagittale', 'coronale', 'assiale', 'slice'\n",
    "    ]\n",
    "    \n",
    "    content_count = sum(1 for term in medical_content if term.lower() in result.lower())\n",
    "    return content_count >= 3\n",
    "\n",
    "def calculate_real_quality_score(result: str) -> int:\n",
    "    \"\"\"Score basato su contenuto reale, non formale\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Penalizza heavily se è solo template\n",
    "    if not validate_actual_analysis(result):\n",
    "        return 0\n",
    "    \n",
    "    # Penalizza ripetizioni\n",
    "    lines = result.split('\\n')\n",
    "    unique_lines = set(line.strip() for line in lines if line.strip())\n",
    "    if len(lines) > 5:\n",
    "        repetition_ratio = len(unique_lines) / len([l for l in lines if l.strip()])\n",
    "        score += int(repetition_ratio * 50)\n",
    "    \n",
    "    # Premia contenuto medico specifico\n",
    "    medical_terms = [\n",
    "        'cortex', 'hippocampus', 'temporal', 'parietal', 'frontal',\n",
    "        'activation', 'heatmap', 'grad-cam', 'alzheimer', 'pathology'\n",
    "    ]\n",
    "    \n",
    "    for term in medical_terms:\n",
    "        if term.lower() in result.lower():\n",
    "            score += 15\n",
    "    \n",
    "    # Premia descrizioni di colori/pattern\n",
    "    visual_terms = ['red', 'blue', 'yellow', 'green', 'bright', 'dark', 'pattern', 'region']\n",
    "    for term in visual_terms:\n",
    "        if term.lower() in result.lower():\n",
    "            score += 10\n",
    "    \n",
    "    # Premia menzioni di slice/axis\n",
    "    slice_terms = ['slice', 'sagittal', 'coronal', 'axial', 'x-axis', 'y-axis', 'z-axis', 'sagittale', 'coronale', 'assiale']\n",
    "    for term in slice_terms:\n",
    "        if term.lower() in result.lower():\n",
    "            score += 20\n",
    "    \n",
    "    # Premia struttura senza template\n",
    "    if any(section in result for section in ['Primary Findings:', 'What I see:', 'Alzheimer', 'OSSERVAZIONI', 'COSA VEDO']) and '[' not in result:\n",
    "        score += 30\n",
    "    \n",
    "    # Premia analisi multi-slice\n",
    "    multi_slice_indicators = ['sagittale', 'coronale', 'assiale', 'x=64', 'y=64', 'z=25']\n",
    "    multi_slice_count = sum(1 for indicator in multi_slice_indicators if indicator.lower() in result.lower())\n",
    "    if multi_slice_count >= 2:\n",
    "        score += 25\n",
    "    \n",
    "    return score\n",
    "\n",
    "def format_long_text(text: str, max_line_length: int = 80) -> str:\n",
    "    \"\"\"Formatta testo lungo andando a capo\"\"\"\n",
    "    existing_lines = text.split('\\n')\n",
    "    formatted_lines = []\n",
    "    \n",
    "    for line in existing_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            formatted_lines.append(\"\")\n",
    "            continue\n",
    "            \n",
    "        if len(line) <= max_line_length:\n",
    "            formatted_lines.append(line)\n",
    "            continue\n",
    "            \n",
    "        words = line.split()\n",
    "        current_line = \"\"\n",
    "        \n",
    "        for word in words:\n",
    "            if len(current_line + \" \" + word) <= max_line_length:\n",
    "                current_line += \" \" + word if current_line else word\n",
    "            else:\n",
    "                if current_line:\n",
    "                    formatted_lines.append(current_line)\n",
    "                current_line = word\n",
    "        \n",
    "        if current_line:\n",
    "            formatted_lines.append(current_line)\n",
    "    \n",
    "    return \"\\n\".join(formatted_lines)\n",
    "\n",
    "def get_unique_filename(output_dir: Path, image_path: str) -> Path:\n",
    "    \"\"\"Genera nome file unico basato sull'immagine\"\"\"\n",
    "    image_name = Path(image_path).stem\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return output_dir / f\"{image_name}_{timestamp}.jsonl\"\n",
    "\n",
    "def save_result_to_jsonl(image_path, prompt, cleaned_result, quality_score=None, prediction_data=None):\n",
    "    \"\"\"Salva risultato in formato JSONL con predizioni integrate\"\"\"\n",
    "    try:\n",
    "        output_dir = Path(r\"C:\\Users\\nicolo.petruzzella\\OneDrive - LUTECH SPA\\Desktop\\promptMRI\\outputs_jsonl\")\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        output_file = get_unique_filename(output_dir, image_path)\n",
    "\n",
    "        formatted_result = format_long_text(cleaned_result, max_line_length=80)\n",
    "        formatted_prompt = format_long_text(prompt, max_line_length=80)\n",
    "        \n",
    "        record = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"image\": image_path,\n",
    "            \"prompt\": formatted_prompt,\n",
    "            \"result\": formatted_result,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"config\": CURRENT_CONFIG,\n",
    "            \"prediction_data\": prediction_data  # Aggiungi predizioni\n",
    "        }\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"{\\n\")\n",
    "            f.write(f'  \"timestamp\": \"{record[\"timestamp\"]}\",\\n')\n",
    "            f.write(f'  \"image\": \"{record[\"image\"]}\",\\n')\n",
    "            f.write('  \"prompt\": \"')\n",
    "            f.write(record[\"prompt\"].replace('\"', '\\\\\"').replace('\\n', '\\\\n'))\n",
    "            f.write('\",\\n')\n",
    "            f.write('  \"result\": \"')\n",
    "            f.write(record[\"result\"].replace('\"', '\\\\\"').replace('\\n', '\\\\n'))\n",
    "            f.write('\",\\n')\n",
    "            f.write(f'  \"quality_score\": {quality_score},\\n')\n",
    "            f.write('  \"config\": ')\n",
    "            f.write(json.dumps(record[\"config\"]))\n",
    "            f.write(',\\n')\n",
    "            f.write('  \"prediction_data\": ')\n",
    "            f.write(json.dumps(record[\"prediction_data\"]))\n",
    "            f.write('\\n}\\n')\n",
    "        \n",
    "        print(f\"✅ Risultato salvato in: {output_file.name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore durante il salvataggio: {e}\")\n",
    "\n",
    "# ==================== PROMPT FUNCTIONS WITH PREDICTIONS ====================\n",
    "\n",
    "def get_multi_slice_optimized_prompt_with_predictions(prediction_info: str = \"\"):\n",
    "    \"\"\"Prompt ottimizzato per analisi multi-slice con predizioni AI integrate\"\"\"\n",
    "    base_prompt = (\n",
    "        \"Stai analizzando uno screenshot che contiene 3 slice MRI cerebrali con sovrapposizione Grad-CAM per rilevazione Alzheimer.\\n\\n\"\n",
    "        \"IMPORTANTE: Analizza l'IMMAGINE REALE che vedi, non ripetere le istruzioni.\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    if prediction_info:\n",
    "        base_prompt += f\"{prediction_info}\\n\\n\"\n",
    "    \n",
    "    base_prompt += (\n",
    "        \"Contesto tecnico:\\n\"\n",
    "        \"- L'immagine mostra 3 sezioni ortogonali del cervello:\\n\"\n",
    "        \"    • Sagittale (x-axis, coordinata X=64): visione laterale del cervello, utile per analizzare strutture profonde come ippocampo e corpo calloso\\n\"\n",
    "        \"    • Coronale (y-axis, coordinata Y=64): visione frontale, cruciale per confrontare emisferi destro/sinistro e osservare simmetrie\\n\"\n",
    "        \"    • Assiale (z-axis, coordinata Z=25): visione dall'alto/inferiore, utile per identificare distribuzioni corticali\\n\"\n",
    "        \"- Rosso/caldo = alta attivazione del modello\\n\"\n",
    "        \"- Blu/freddo = bassa attivazione\\n\\n\"\n",
    "        \"Fornisci la tua analisi seguendo ESATTAMENTE questa struttura:\\n\\n\"\n",
    "        \"🧠 Primary Findings:\\n\"\n",
    "        \"• Descrivi quello che vedi effettivamente in ciascuna delle 3 slice\\n\"\n",
    "        \"• Specifica colori, intensità e posizione anatomica precisa\\n\"\n",
    "        \"• Nota differenze significative tra le slice\\n\\n\"\n",
    "        \"🔬 Alzheimer's Indicators:\\n\"\n",
    "        \"• Le attivazioni interessano ippocampo, corteccia entorinale, o aree tipiche AD?\\n\"\n",
    "        \"• Il pattern è coerente con neurodegenerazione?\\n\"\n",
    "        \"• Ci sono asimmetrie tra emisferi? In quale piano?\\n\"\n",
    "    )\n",
    "    \n",
    "    if prediction_info:\n",
    "        base_prompt += \"• Come si allinea l'analisi visiva con la predizione del modello AI?\\n\"\n",
    "    \n",
    "    base_prompt += (\n",
    "        \"\\n📈 Confidence Level:\\n\"\n",
    "        \"• Alto/Medio/Basso - Spiega basandoti su chiarezza dell'immagine e pattern\\n\"\n",
    "    )\n",
    "    \n",
    "    if prediction_info:\n",
    "        base_prompt += \"• Considera anche l'affidabilità delle metriche del modello AI\\n\"\n",
    "    \n",
    "    base_prompt += (\n",
    "        \"\\n📌 Recommended Actions:\\n\"\n",
    "        \"• Altre slice necessarie? Specifica asse e coordinate\\n\"\n",
    "        \"• Esami clinici aggiuntivi suggeriti\\n\"\n",
    "        \"• Prossimi step diagnostici\\n\\n\"\n",
    "        \"Descrivi solo quello che vedi realmente nell'immagine.\"\n",
    "    )\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "def get_multi_slice_robust_prompt_v2_with_predictions(prediction_info: str = \"\"):\n",
    "    \"\"\"Prompt robusto con predizioni integrate\"\"\"\n",
    "    base_prompt = (\n",
    "        \"Stai osservando un'immagine che mostra 1, 2 o 3 slice MRI cerebrali con sovrapposizione Grad-CAM, usate per l'analisi di Alzheimer.\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    if prediction_info:\n",
    "        base_prompt += f\"{prediction_info}\\n\\n\"\n",
    "    \n",
    "    base_prompt += (\n",
    "        \"🧠 Attenzione:\\n\"\n",
    "        \"- L'immagine attuale mostra slice alle seguenti coordinate:\\n\"\n",
    "        \"   • Sagittale (x-axis): x = 64\\n\"\n",
    "        \"   • Coronale (y-axis): y = 64\\n\"\n",
    "        \"   • Assiale (z-axis): z = 25\\n\"\n",
    "        \"- Le coordinate possibili in questo esame sono:\\n\"\n",
    "        \"   • x ∈ [0, 127] (sagittale)\\n\"\n",
    "        \"   • y ∈ [0, 127] (coronale)\\n\"\n",
    "        \"   • z ∈ [0, 49]  (assiale)\\n\"\n",
    "        \"- Ogni slice rappresenta una vista ortogonale del cervello:\\n\"\n",
    "        \"   • Sagittale (x): visione laterale\\n\"\n",
    "        \"   • Coronale (y): visione frontale\\n\"\n",
    "        \"   • Assiale (z): visione dall'alto o inferiore\\n\"\n",
    "        \"- Colori caldi (rosso/giallo) = alta attivazione del modello\\n\"\n",
    "        \"- Colori freddi (blu) = bassa attivazione\\n\\n\"\n",
    "        \"✏️ Analizza seguendo questa struttura, ma **usa parole tue**:\\n\\n\"\n",
    "        \"🔍 Slice Analysis:\\n\"\n",
    "        \"• Quante slice vedi chiaramente?\\n\"\n",
    "        \"• Che orientamento sembrano avere (x, y, z)? Dove sono posizionate visivamente?\\n\"\n",
    "        \"• Cosa mostrano a livello di attivazioni?\\n\\n\"\n",
    "        \"🧠 Alzheimer's Indicators:\\n\"\n",
    "        \"• Le attivazioni coinvolgono regioni come ippocampo o corteccia entorinale?\\n\"\n",
    "        \"• Il pattern è coerente con Alzheimer o altre forme di neurodegenerazione?\\n\"\n",
    "        \"• Noti asimmetrie tra emisferi?\\n\"\n",
    "    )\n",
    "    \n",
    "    if prediction_info:\n",
    "        base_prompt += \"• L'analisi visiva conferma o contraddice la predizione del modello?\\n\"\n",
    "    \n",
    "    base_prompt += (\n",
    "        \"\\n📈 Confidence Level:\\n\"\n",
    "        \"• Valuta il tuo livello di sicurezza (Alto / Medio / Basso) e spiega il perché\\n\"\n",
    "    )\n",
    "    \n",
    "    if prediction_info:\n",
    "        base_prompt += \"• Considera l'affidabilità delle metriche AI nella tua valutazione\\n\"\n",
    "    \n",
    "    base_prompt += (\n",
    "        \"\\n📌 Azioni Consigliate:\\n\"\n",
    "        \"• Servono altre slice? Di quale asse e a quale coordinata (se lo ritieni utile)?\\n\"\n",
    "        \"• Se opportuno, suggerisci anche coordinate alternative rispetto a quelle mostrate (es. x, y, z diverse), motivando la scelta.\\n\"\n",
    "        \"• Suggerisci esami clinici o neuroimaging aggiuntivi\\n\"\n",
    "        \"• Indica i prossimi passi per una diagnosi più precisa\\n\\n\"\n",
    "        \"🧠 **Stima dello stadio della malattia:**\\n\"\n",
    "        \"• Sulla base delle attivazioni visibili\"\n",
    "    )\n",
    "    \n",
    "    if prediction_info:\n",
    "        base_prompt += \" e della predizione del modello\"\n",
    "    \n",
    "    base_prompt += (\n",
    "        \", indica in quale fase potremmo trovarci:\\n\"\n",
    "        \"   - CN (Cognitivamente Normale)\\n\"\n",
    "        \"   - MCI (Mild Cognitive Impairment)\\n\"\n",
    "        \"   - s-MCI (MCI stabile)\\n\"\n",
    "        \"   - c-MCI (MCI con progressione verso Alzheimer)\\n\"\n",
    "        \"   - AD (Malattia di Alzheimer conclamata)\\n\"\n",
    "        \"• Motiva brevemente la tua valutazione.\\n\\n\"\n",
    "        \"❗Non ripetere queste istruzioni. Concentrati solo sull'immagine reale che stai vedendo.\"\n",
    "    )\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "def select_prompt_with_predictions(prediction_info: str = \"\"):\n",
    "    \"\"\"Seleziona quale prompt usare, con predizioni integrate\"\"\"\n",
    "    prompts = {\n",
    "        \"1\": (\"Multi-Slice Ottimizzato CON Predizioni (RACCOMANDATO)\", \n",
    "              get_multi_slice_optimized_prompt_with_predictions(prediction_info)),\n",
    "        \"2\": (\"Diretto Visuale CON Predizioni (v2)\", \n",
    "              get_multi_slice_robust_prompt_v2_with_predictions(prediction_info)),\n",
    "        \"3\": (\"Multi-Slice Ottimizzato SENZA Predizioni\", \n",
    "              get_multi_slice_optimized_prompt_with_predictions(\"\")),\n",
    "        \"4\": (\"Diretto Visuale SENZA Predizioni\", \n",
    "              get_multi_slice_robust_prompt_v2_with_predictions(\"\")),\n",
    "    }\n",
    "    \n",
    "    print(\"🧠 Seleziona il prompt da usare:\")\n",
    "    for key, (name, _) in prompts.items():\n",
    "        print(f\"{key}. {name}\")\n",
    "    \n",
    "    choice = input(\"\\nScegli (1-4, default=1): \").strip()\n",
    "    \n",
    "    if choice not in prompts:\n",
    "        choice = \"1\"  # Default al primo\n",
    "    \n",
    "    selected_name, selected_prompt = prompts[choice]\n",
    "    print(f\"✅ Selezionato: {selected_name}\\n\")\n",
    "    \n",
    "    return selected_prompt\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "\n",
    "def main_with_predictions():\n",
    "    \"\"\"Main function integrata con API predizioni\"\"\"\n",
    "    # Immagine di test\n",
    "    img_file = \"C:/Users/nicolo.petruzzella/OneDrive - LUTECH SPA/Desktop/promptMRI/imagesPrompt/z/Screenshot 2025-07-10 112906.png\"\n",
    "    \n",
    "    print(\"🧠 Analisi MRI con Grad-CAM per Alzheimer (CON PREDIZIONI AI)\\n\")\n",
    "    \n",
    "    # Inizializza servizio predizioni\n",
    "    prediction_service = BrainMedPredictionService()\n",
    "    \n",
    "    # ID paziente di esempio (sostituisci con quello reale)\n",
    "    patient_id = \"40a7bc47-e0ca-41ec-bb17-467957548be0\"\n",
    "    \n",
    "    # Opzione per includere predizioni\n",
    "    use_predictions = input(\"🤖 Vuoi includere le predizioni AI? (y/n, default=y): \").strip().lower()\n",
    "    if use_predictions == '' or use_predictions == 'y':\n",
    "        print(f\"📡 Recupero predizioni per paziente: {patient_id}\")\n",
    "        prediction_data = prediction_service.get_patient_prediction(patient_id)\n",
    "        \n",
    "        if prediction_data:\n",
    "            print(\"✅ Predizioni recuperate!\")\n",
    "            print(f\"🎯 Classe predetta: {prediction_data.get('class_disease', 'N/A')}\")\n",
    "            print(f\"📊 Accuracy: {prediction_data.get('accuracy', 0):.1%}\")\n",
    "            \n",
    "            # Opzione per includere metriche\n",
    "            include_metrics = input(\"📊 Includere metriche nel prompt? (y/n, default=y): \").strip().lower()\n",
    "            include_metrics = include_metrics == '' or include_metrics == 'y'\n",
    "            \n",
    "            prediction_info = prediction_service.format_prediction_for_prompt(\n",
    "                prediction_data, include_metrics\n",
    "            )\n",
    "        else:\n",
    "            print(\"❌ Impossibile recuperare predizioni. Procedo senza.\")\n",
    "            prediction_info = \"\"\n",
    "            prediction_data = None\n",
    "    else:\n",
    "        prediction_info = \"\"\n",
    "        prediction_data = None\n",
    "    \n",
    "    # Selezione prompt con predizioni\n",
    "    selected_prompt = select_prompt_with_predictions(prediction_info)\n",
    "    \n",
    "    # Selezione configurazione\n",
    "    print(\"⚙️ Seleziona configurazione:\")\n",
    "    for key, config in CONFIGS.items():\n",
    "        print(f\"- {key}: temp={config['temperature']}, tokens={config['max_tokens']}\")\n",
    "    \n",
    "    config_choice = input(\"\\nScegli configurazione (conservative/balanced/creative, default=conservative): \").strip()\n",
    "    if config_choice in CONFIGS:\n",
    "        global CURRENT_CONFIG\n",
    "        CURRENT_CONFIG = CONFIGS[config_choice]\n",
    "        print(f\"✅ Configurazione: {config_choice}\")\n",
    "    else:\n",
    "        print(\"✅ Configurazione: conservative (default)\")\n",
    "    \n",
    "    print(f\"\\n🔧 Parametri: temp={CURRENT_CONFIG['temperature']}, tokens={CURRENT_CONFIG['max_tokens']}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        result = predict_image(\n",
    "            img_file,\n",
    "            selected_prompt,\n",
    "            max_tokens=CURRENT_CONFIG['max_tokens'],\n",
    "            temperature=CURRENT_CONFIG['temperature'],\n",
    "            top_p=CURRENT_CONFIG['top_p'],\n",
    "            do_sample=CURRENT_CONFIG['do_sample']\n",
    "        )\n",
    "        \n",
    "        result_clean = clean_response_v2(result)\n",
    "        \n",
    "        # Validazione\n",
    "        is_valid = validate_actual_analysis(result_clean)\n",
    "        quality_score = calculate_real_quality_score(result_clean)\n",
    "        \n",
    "        print(f\"📊 Risultato:\")\n",
    "        print(f\"✅ Valido: {is_valid}\")\n",
    "        print(f\"🏆 Score: {quality_score}\")\n",
    "        print(f\"📏 Lunghezza: {len(result_clean)} caratteri\")\n",
    "        \n",
    "        if prediction_data:\n",
    "            print(f\"🤖 Predizione AI: Classe {prediction_data.get('class_disease', 'N/A')}\")\n",
    "            print(f\"📊 Accuracy: {prediction_data.get('accuracy', 0):.1%}\")\n",
    "        \n",
    "        print(f\"\\n📋 Analisi completa:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(result_clean)\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Salvataggio con predizioni\n",
    "        save_result_to_jsonl(img_file, selected_prompt, result_clean, quality_score, prediction_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore durante l'analisi: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_with_predictions()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
